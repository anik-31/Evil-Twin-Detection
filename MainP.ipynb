{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d39687-379a-4ba8-a63a-94620a653f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import time\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from queue import Queue\n",
    "import Levenshtein  # Ensure this library is installed\n",
    "from collections import Counter\n",
    "\n",
    "# Load the saved model and scaler\n",
    "model = joblib.load('evil_twin_rf_model.pkl')\n",
    "scaler = joblib.load('evil_twin_scaler.pkl')\n",
    "\n",
    "# Parameters\n",
    "SCAN_THRESHOLD = 10  # Number of scans before classification\n",
    "RSSI_VARIATION_THRESHOLD = 20  # RSSI fluctuation threshold to label as suspicious\n",
    "\n",
    "# Shared resources\n",
    "accumulated_scan_data = []\n",
    "scan_counter = 0\n",
    "network_history = {}\n",
    "network_classification = {}\n",
    "shared_results = Queue()  # Thread-safe queue for sharing data between threads\n",
    "processed_networks = {}  # {SSID_BSSID: 'Legitimate'}\n",
    "all_results = []  # Store results for all 10 scans\n",
    "\n",
    "data_lock = threading.Lock()  # Lock for thread-safe access to shared resources\n",
    "\n",
    "# Debug logging function\n",
    "def debug_log(message):\n",
    "    print(f\"[DEBUG] {datetime.now().strftime('%H:%M:%S')}: {message}\")\n",
    "\n",
    "# Function to parse ESP data\n",
    "def parse_esp_data(raw_data):\n",
    "    debug_log(f\"Raw data received: {raw_data}\")\n",
    "    try:\n",
    "        if not raw_data or raw_data.startswith(\"Number of networks\"):\n",
    "            return []\n",
    "\n",
    "        lines = raw_data.strip().split(\"\\n\")\n",
    "        parsed_networks = []\n",
    "\n",
    "        for line in lines:\n",
    "            fields = line.split(',')\n",
    "            if len(fields) < 12:\n",
    "                debug_log(f\"Incomplete data ignored: {line}\")\n",
    "                continue\n",
    "\n",
    "            parsed_data = {\n",
    "                \"SSID\": fields[1],\n",
    "                \"BSSID\": fields[3],\n",
    "                \"RSSI\": int(fields[5]),\n",
    "                \"Channel\": int(fields[7]),\n",
    "                \"Encryption\": int(fields[9]),\n",
    "                \"Time\": fields[11]\n",
    "            }\n",
    "            parsed_networks.append(parsed_data)\n",
    "\n",
    "        debug_log(f\"Parsed {len(parsed_networks)} networks.\")\n",
    "        return parsed_networks\n",
    "    except Exception as e:\n",
    "        debug_log(f\"Error parsing data: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to calculate additional features for classification\n",
    "def calculate_features(network_data):\n",
    "    ssid_bssid_key = f\"{network_data['SSID']}_{network_data['BSSID']}\"\n",
    "    current_time = datetime.strptime(network_data[\"Time\"], \"%H:%M:%S\")\n",
    "    rssi = network_data[\"RSSI\"]\n",
    "\n",
    "    if ssid_bssid_key not in network_history:\n",
    "        network_history[ssid_bssid_key] = {\n",
    "            \"SSID\": network_data[\"SSID\"],\n",
    "            \"BSSID\": network_data[\"BSSID\"],\n",
    "            \"RSSI_values\": [],\n",
    "            \"Timestamps\": [],\n",
    "            \"Channel\": network_data[\"Channel\"],\n",
    "            \"Encryption\": network_data[\"Encryption\"]\n",
    "        }\n",
    "\n",
    "    history = network_history[ssid_bssid_key]\n",
    "    history[\"RSSI_values\"].append(rssi)\n",
    "    history[\"Timestamps\"].append(current_time)\n",
    "\n",
    "    levenshtein_distance = 0  # Placeholder\n",
    "    detection_frequency = len(history[\"Timestamps\"])\n",
    "    rssi_variance = np.var(history[\"RSSI_values\"])\n",
    "\n",
    "    if len(history[\"Timestamps\"]) > 1:\n",
    "        time_diff = (history[\"Timestamps\"][-1] - history[\"Timestamps\"][-2]).total_seconds()\n",
    "        rssi_diff = history[\"RSSI_values\"][-1] - history[\"RSSI_values\"][-2]\n",
    "        signal_gradient = rssi_diff / time_diff if time_diff > 0 else 0\n",
    "    else:\n",
    "        signal_gradient = 0\n",
    "\n",
    "    signal_fluctuation = max(history[\"RSSI_values\"]) - min(history[\"RSSI_values\"])\n",
    "    encryption_hierarchy = history[\"Encryption\"]\n",
    "    congested_channel = False\n",
    "\n",
    "    feature_vector = [\n",
    "        rssi,\n",
    "        history[\"Channel\"],\n",
    "        levenshtein_distance,\n",
    "        detection_frequency,\n",
    "        rssi_variance,\n",
    "        signal_gradient,\n",
    "        signal_fluctuation,\n",
    "        encryption_hierarchy,\n",
    "        int(congested_channel)\n",
    "    ]\n",
    "\n",
    "    debug_log(f\"Feature Vector: {feature_vector}\")\n",
    "    return feature_vector\n",
    "\n",
    "# Function to classify network using the trained model\n",
    "def classify_network(feature_vector):\n",
    "    try:\n",
    "        debug_log(f\"Input Features: {feature_vector}\")\n",
    "        feature_vector_scaled = scaler.transform([feature_vector])\n",
    "        prediction = model.predict(feature_vector_scaled)\n",
    "        debug_log(f\"Prediction: {prediction}\")\n",
    "        return int(prediction[0])  # 0: Legitimate, 1: Evil Twin\n",
    "    except Exception as e:\n",
    "        debug_log(f\"Error in classification: {e}\")\n",
    "        return 0  # Default to Legitimate on error\n",
    "\n",
    "# Function to process and classify after accumulating data for SCAN_THRESHOLD scans\n",
    "def classify_after_scans():\n",
    "    global scan_counter, accumulated_scan_data, all_results\n",
    "    with data_lock:\n",
    "        scan_counter += 1\n",
    "        debug_log(f\"Scan counter: {scan_counter}, accumulated data size: {len(accumulated_scan_data)}\")\n",
    "\n",
    "        if scan_counter <= 3:\n",
    "            results = [\n",
    "                {**network, \"Classification\": \"Legitimate\"}\n",
    "                for network in accumulated_scan_data\n",
    "            ]\n",
    "            debug_log(f\"First 3 scans, marking all networks as legitimate: {len(results)} results.\")\n",
    "            for network in accumulated_scan_data:\n",
    "                ssid_bssid_key = f\"{network['SSID']}_{network['BSSID']}\"\n",
    "                processed_networks[ssid_bssid_key] = \"Legitimate\"\n",
    "            all_results.extend(results)\n",
    "\n",
    "        else:\n",
    "            results = []\n",
    "            for network in accumulated_scan_data:\n",
    "                ssid_bssid_key = f\"{network['SSID']}_{network['BSSID']}\"\n",
    "                if ssid_bssid_key in processed_networks:\n",
    "                    continue\n",
    "\n",
    "                feature_vector = calculate_features(network)\n",
    "                prediction = classify_network(feature_vector)\n",
    "                classification = 'Evil Twin' if prediction == 1 else 'Legitimate'\n",
    "\n",
    "                processed_networks[ssid_bssid_key] = classification\n",
    "                results.append({**network, \"Classification\": classification})\n",
    "\n",
    "            debug_log(f\"Post scan 3, newly classified networks: {len(results)} results.\")\n",
    "            all_results.extend(results)\n",
    "\n",
    "        if scan_counter == SCAN_THRESHOLD:\n",
    "            shared_results.put(all_results)\n",
    "            debug_log(f\"Scan threshold reached. Shared {len(all_results)} results.\")\n",
    "            all_results = []  # Reset for the next round\n",
    "\n",
    "        accumulated_scan_data.clear()\n",
    "\n",
    "# Serial reading function\n",
    "def read_from_esp(port, baudrate=115200):\n",
    "    try:\n",
    "        with serial.Serial(port, baudrate, timeout=1) as ser:\n",
    "            current_scan_data = []\n",
    "            while True:\n",
    "                if ser.in_waiting > 0:\n",
    "                    raw_data = ser.readline().decode('utf-8', errors='replace').strip()\n",
    "                    debug_log(f\"ESP Output: {raw_data}\")\n",
    "\n",
    "                    if raw_data.startswith(\"Number of networks\"):\n",
    "                        if current_scan_data:\n",
    "                            with data_lock:\n",
    "                                accumulated_scan_data.extend(current_scan_data)\n",
    "                            classify_after_scans()\n",
    "                            current_scan_data = []\n",
    "                        continue\n",
    "\n",
    "                    networks = parse_esp_data(raw_data)\n",
    "                    if networks:\n",
    "                        current_scan_data.extend(networks)\n",
    "    except serial.SerialException as e:\n",
    "        debug_log(f\"Serial Error: {e}\")\n",
    "    except Exception as e:\n",
    "        debug_log(f\"Unexpected Error: {e}\")\n",
    "\n",
    "# Streamlit app function\n",
    "def main():\n",
    "    st.title(\"Evil Twin Detection\")\n",
    "    st.markdown(\"### Real-Time Network Classification\")\n",
    "\n",
    "    # Use session state to store results\n",
    "    if 'results' not in st.session_state:\n",
    "        st.session_state.results = []\n",
    "\n",
    "    placeholder = st.empty()\n",
    "\n",
    "    while True:\n",
    "        if not shared_results.empty():\n",
    "            new_results = shared_results.get()\n",
    "            st.session_state.results.extend(new_results)\n",
    "\n",
    "            debug_log(f\"Streamlit received {len(new_results)} entries.\")\n",
    "\n",
    "        # Display updated results in a DataFrame\n",
    "        if st.session_state.results:\n",
    "            df = pd.DataFrame(st.session_state.results)\n",
    "            with placeholder.container():\n",
    "                st.dataframe(df)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Start serial reading thread\n",
    "esp_thread = threading.Thread(target=read_from_esp, args=('COM5', 115200), daemon=True)\n",
    "esp_thread.start()\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
